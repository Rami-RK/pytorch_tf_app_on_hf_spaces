{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rami-RK/pytorch_tf_app_on_hf_spaces/blob/main/Gradio_app_with_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Saving a Tensorflow trained model and integration with Gradio APP on HF Spaces**"
      ],
      "metadata": {
        "id": "PprHHREe8n5R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-1SXdIFtYAY"
      },
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "1. Understand Hugging Face Spaces\n",
        "2. Deploy custom model on spaces with Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syM6r3KAwLRb"
      },
      "source": [
        "## Building an Image Classification model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrqghDRkvDAh"
      },
      "source": [
        "We know how to build a simple CNN, let's build and train one to solve an image classification problem.\n",
        "\n",
        "We will work with the cats-vs-dogs dataset to classify whether a given image is that of a cat or a dog .i.e a  binary classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "gLFh-pFLUdpK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XelqVXc1wRv8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already uploaded the dataset into structured folders. You simply need to download it from our repository."
      ],
      "metadata": {
        "id": "zCNRgUsjdd6U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pyM-hyYSkiu0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download the data\n",
        "!wget -O cats_vs_dogs_small.zip -qq https://www.dropbox.com/scl/fi/wiaspjsrue17jbtkp6vga/cats_vs_dogs_small.zip?rlkey=4798vwv7v75bihwjxye0tr7c3&dl=0\n",
        "!unzip -qq '/content/cats_vs_dogs_small.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TUfhxMyOkrA2"
      },
      "outputs": [],
      "source": [
        "# defining path names for futur use\n",
        "data_dir = '/content/cats_vs_dogs_small'\n",
        "\n",
        "train_path = data_dir + '/train'\n",
        "validation_path = data_dir + '/validation'\n",
        "test_path = data_dir + '/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3FyIB1gyqXg"
      },
      "source": [
        "### Converting the image dataset into a workable format\n",
        "\n",
        "We have the images in folders. We need to make it into a workable dataset:\n",
        "  * Which has labels\n",
        "  * All the images have the same size\n",
        "\n",
        "For this, we will use the utility [**image_dataset_from_directory**](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory).\n",
        "\n",
        "Calling image_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CVOWgUhECt5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f57a2b0e-01b7-4b1d-8c22-2d64522df3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 2000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = image_dataset_from_directory(\n",
        "               train_path,\n",
        "                image_size=(180, 180), # Resize the images to (180,180)\n",
        "                batch_size=32,\n",
        "                class_names=['cat','dog'],)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "                      validation_path,\n",
        "                      image_size=(180, 180),\n",
        "                      batch_size=32,\n",
        "                class_names=['cat','dog'],)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "                test_path,\n",
        "                image_size=(180, 180),\n",
        "                batch_size=32,\n",
        "                class_names=['cat','dog'],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w_UlcX7j7c8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7794415b-c78c-4b5d-b8f0-086fe7031230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset = <_BatchDataset element_spec=(TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "print(f\"train_dataset = {train_dataset}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISmvr0Zx2-fP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aaea23a-8017-49e0-88b0-d3b3f2576b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data batch shape: (32, 180, 180, 3)\n",
            "labels batch shape: (32,)\n"
          ]
        }
      ],
      "source": [
        "# Verify batch size\n",
        "for data_batch, labels_batch in train_dataset:\n",
        "  print(\"data batch shape:\", data_batch.shape)\n",
        "  print(\"labels batch shape:\", labels_batch.shape)\n",
        "  break\n",
        "# Q: What is the batch size of each mini-batch? A: 32"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Call Back Function"
      ],
      "metadata": {
        "id": "tIL3mec732C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to return a commmonly used callback_list\n",
        "def def_callbacks(filepath, mod_chk_mon = \"val_loss\", tensorboard = True, earlystop = 0 ):\n",
        "    callback_list = []\n",
        "\n",
        "    # Defualt callback\n",
        "    callback_list.append(keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                         save_best_only = True,\n",
        "                                         monitor=mod_chk_mon))\n",
        "    if tensorboard:\n",
        "      log_dir = \"tensorLog_\" + filepath\n",
        "      callback_list.append(keras.callbacks.TensorBoard(log_dir=log_dir))\n",
        "\n",
        "    if earlystop>0:\n",
        "       callback_list.append(keras.callbacks.EarlyStopping(patience=earlystop))\n",
        "\n",
        "    return callback_list"
      ],
      "metadata": {
        "id": "Sjcq2pDn3zpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQqCNE1H8IRf"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMU298TB2XiY"
      },
      "source": [
        "The small dataset can cause a high variance estimation of model performance\n",
        "\n",
        "Q: How to overcome this and get a more robust model?\n",
        "\n",
        "Now, we want to avoid this problem altogether by artificially (and cleverly) producing new data from the already available data.\n",
        "\n",
        "For this, we perform **data augmentation**.\n",
        "\n",
        "Data augmentation is another regularization method. What other methods did we see in the last tutorial?\n",
        "\n",
        "Data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples via a number of random transformations that yield a believable-looking image. Common transformations include:\n",
        "  * Flipping the image\n",
        "  * Rotating the image\n",
        "  * Zooming in/out of the image\n",
        "\n",
        "See some sample images below after augmentation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OANl64VG5vCI"
      },
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1HRhsHEHtcVptNVMF1EbCGiZX5XuTdrs5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gRMWTEGhNQk"
      },
      "outputs": [],
      "source": [
        "# Performing the data augmentation as series of transformations\n",
        "def get_data_augmented(flip=\"horizontal\",rotation=0.1,zoom=0.2):\n",
        "    data_augmentation = keras.Sequential([\n",
        "      keras.layers.RandomFlip(flip),\n",
        "      keras.layers.RandomRotation(rotation),\n",
        "      keras.layers.RandomZoom(zoom)])\n",
        "    return data_augmentation\n",
        "# Q: what does the above function return? A: A sequence of layers\n",
        "\n",
        "data_augmentation = get_data_augmented()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fftU_EPahV48"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "# Augmenting data - Transformations of images by random factors\n",
        "# so the the network never sees the same data twice\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)     # Q: Dropout is a _______ method\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHYpxJ5IhdFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0425d4dd-2619-432b-ff3a-503371cf8e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/144\n",
            "63/63 [==============================] - 12s 90ms/step - loss: 0.6987 - accuracy: 0.4820 - val_loss: 0.6927 - val_accuracy: 0.5000\n",
            "Epoch 2/144\n",
            "63/63 [==============================] - 7s 100ms/step - loss: 0.7304 - accuracy: 0.4950 - val_loss: 0.6960 - val_accuracy: 0.5000\n",
            "Epoch 3/144\n",
            "63/63 [==============================] - 5s 72ms/step - loss: 0.6930 - accuracy: 0.5060 - val_loss: 0.6910 - val_accuracy: 0.5040\n",
            "Epoch 4/144\n",
            "63/63 [==============================] - 6s 89ms/step - loss: 0.6936 - accuracy: 0.5460 - val_loss: 0.6787 - val_accuracy: 0.6480\n",
            "Epoch 5/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.6895 - accuracy: 0.5940 - val_loss: 0.6437 - val_accuracy: 0.6080\n",
            "Epoch 6/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.6578 - accuracy: 0.6215 - val_loss: 0.6277 - val_accuracy: 0.6320\n",
            "Epoch 7/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.6405 - accuracy: 0.6415 - val_loss: 0.6573 - val_accuracy: 0.5820\n",
            "Epoch 8/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.6143 - accuracy: 0.6720 - val_loss: 0.5903 - val_accuracy: 0.6610\n",
            "Epoch 9/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.6085 - accuracy: 0.6645 - val_loss: 1.0281 - val_accuracy: 0.5560\n",
            "Epoch 10/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.6001 - accuracy: 0.6735 - val_loss: 0.6079 - val_accuracy: 0.6530\n",
            "Epoch 11/144\n",
            "63/63 [==============================] - 6s 89ms/step - loss: 0.5927 - accuracy: 0.6860 - val_loss: 0.7103 - val_accuracy: 0.6610\n",
            "Epoch 12/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.5881 - accuracy: 0.6845 - val_loss: 0.7575 - val_accuracy: 0.6540\n",
            "Epoch 13/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.5823 - accuracy: 0.6965 - val_loss: 0.5725 - val_accuracy: 0.7010\n",
            "Epoch 14/144\n",
            "63/63 [==============================] - 5s 83ms/step - loss: 0.5674 - accuracy: 0.7060 - val_loss: 0.5706 - val_accuracy: 0.6990\n",
            "Epoch 15/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.5628 - accuracy: 0.7190 - val_loss: 0.5429 - val_accuracy: 0.7140\n",
            "Epoch 16/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.5530 - accuracy: 0.7170 - val_loss: 0.6903 - val_accuracy: 0.6190\n",
            "Epoch 17/144\n",
            "63/63 [==============================] - 6s 87ms/step - loss: 0.5519 - accuracy: 0.7115 - val_loss: 0.5313 - val_accuracy: 0.7400\n",
            "Epoch 18/144\n",
            "63/63 [==============================] - 6s 91ms/step - loss: 0.5389 - accuracy: 0.7265 - val_loss: 0.5016 - val_accuracy: 0.7630\n",
            "Epoch 19/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.5249 - accuracy: 0.7385 - val_loss: 0.5207 - val_accuracy: 0.7440\n",
            "Epoch 20/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.5249 - accuracy: 0.7385 - val_loss: 0.6713 - val_accuracy: 0.7180\n",
            "Epoch 21/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.5050 - accuracy: 0.7525 - val_loss: 0.5741 - val_accuracy: 0.7210\n",
            "Epoch 22/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.5203 - accuracy: 0.7455 - val_loss: 0.5718 - val_accuracy: 0.7220\n",
            "Epoch 23/144\n",
            "63/63 [==============================] - 6s 92ms/step - loss: 0.4967 - accuracy: 0.7515 - val_loss: 0.6223 - val_accuracy: 0.6770\n",
            "Epoch 24/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.4795 - accuracy: 0.7675 - val_loss: 0.4809 - val_accuracy: 0.7750\n",
            "Epoch 25/144\n",
            "63/63 [==============================] - 5s 67ms/step - loss: 0.4775 - accuracy: 0.7635 - val_loss: 0.4888 - val_accuracy: 0.7600\n",
            "Epoch 26/144\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.4683 - accuracy: 0.7770 - val_loss: 0.7756 - val_accuracy: 0.6290\n",
            "Epoch 27/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.4727 - accuracy: 0.7755 - val_loss: 0.4631 - val_accuracy: 0.7790\n",
            "Epoch 28/144\n",
            "63/63 [==============================] - 6s 88ms/step - loss: 0.4694 - accuracy: 0.7850 - val_loss: 0.4684 - val_accuracy: 0.7760\n",
            "Epoch 29/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.4413 - accuracy: 0.7950 - val_loss: 0.4948 - val_accuracy: 0.7720\n",
            "Epoch 30/144\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.4339 - accuracy: 0.8040 - val_loss: 0.4305 - val_accuracy: 0.8020\n",
            "Epoch 31/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.4335 - accuracy: 0.7990 - val_loss: 0.4382 - val_accuracy: 0.7980\n",
            "Epoch 32/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.4079 - accuracy: 0.8125 - val_loss: 0.4621 - val_accuracy: 0.7780\n",
            "Epoch 33/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.4051 - accuracy: 0.8125 - val_loss: 0.4522 - val_accuracy: 0.7900\n",
            "Epoch 34/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.4085 - accuracy: 0.8170 - val_loss: 0.4472 - val_accuracy: 0.7970\n",
            "Epoch 35/144\n",
            "63/63 [==============================] - 5s 76ms/step - loss: 0.3927 - accuracy: 0.8350 - val_loss: 0.4885 - val_accuracy: 0.7660\n",
            "Epoch 36/144\n",
            "63/63 [==============================] - 5s 76ms/step - loss: 0.3908 - accuracy: 0.8255 - val_loss: 0.4252 - val_accuracy: 0.8120\n",
            "Epoch 37/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.3680 - accuracy: 0.8355 - val_loss: 0.5443 - val_accuracy: 0.7960\n",
            "Epoch 38/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.3747 - accuracy: 0.8385 - val_loss: 0.4372 - val_accuracy: 0.8060\n",
            "Epoch 39/144\n",
            "63/63 [==============================] - 6s 90ms/step - loss: 0.3606 - accuracy: 0.8505 - val_loss: 0.4455 - val_accuracy: 0.8120\n",
            "Epoch 40/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.3618 - accuracy: 0.8350 - val_loss: 0.4909 - val_accuracy: 0.8160\n",
            "Epoch 41/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.3615 - accuracy: 0.8410 - val_loss: 0.6148 - val_accuracy: 0.7580\n",
            "Epoch 42/144\n",
            "63/63 [==============================] - 6s 87ms/step - loss: 0.3417 - accuracy: 0.8550 - val_loss: 0.4415 - val_accuracy: 0.8140\n",
            "Epoch 43/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.3478 - accuracy: 0.8515 - val_loss: 0.5499 - val_accuracy: 0.7700\n",
            "Epoch 44/144\n",
            "63/63 [==============================] - 6s 88ms/step - loss: 0.3335 - accuracy: 0.8505 - val_loss: 0.4240 - val_accuracy: 0.8250\n",
            "Epoch 45/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.3167 - accuracy: 0.8575 - val_loss: 0.4945 - val_accuracy: 0.8180\n",
            "Epoch 46/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.3094 - accuracy: 0.8705 - val_loss: 0.4520 - val_accuracy: 0.7930\n",
            "Epoch 47/144\n",
            "63/63 [==============================] - 5s 80ms/step - loss: 0.2916 - accuracy: 0.8780 - val_loss: 0.5968 - val_accuracy: 0.7710\n",
            "Epoch 48/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.3189 - accuracy: 0.8585 - val_loss: 0.4643 - val_accuracy: 0.8230\n",
            "Epoch 49/144\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.2934 - accuracy: 0.8645 - val_loss: 0.4934 - val_accuracy: 0.8350\n",
            "Epoch 50/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.2755 - accuracy: 0.8915 - val_loss: 0.5704 - val_accuracy: 0.8270\n",
            "Epoch 51/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.2816 - accuracy: 0.8850 - val_loss: 0.4870 - val_accuracy: 0.8370\n",
            "Epoch 52/144\n",
            "63/63 [==============================] - 6s 93ms/step - loss: 0.3027 - accuracy: 0.8715 - val_loss: 0.4076 - val_accuracy: 0.8230\n",
            "Epoch 53/144\n",
            "63/63 [==============================] - 6s 89ms/step - loss: 0.2738 - accuracy: 0.8875 - val_loss: 0.5376 - val_accuracy: 0.8010\n",
            "Epoch 54/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.2524 - accuracy: 0.9035 - val_loss: 0.9370 - val_accuracy: 0.7700\n",
            "Epoch 55/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.2736 - accuracy: 0.8880 - val_loss: 0.4697 - val_accuracy: 0.8250\n",
            "Epoch 56/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.2544 - accuracy: 0.8950 - val_loss: 0.5054 - val_accuracy: 0.8170\n",
            "Epoch 57/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.2494 - accuracy: 0.9030 - val_loss: 0.5549 - val_accuracy: 0.8290\n",
            "Epoch 58/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.2428 - accuracy: 0.9040 - val_loss: 0.5699 - val_accuracy: 0.7930\n",
            "Epoch 59/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.2198 - accuracy: 0.9105 - val_loss: 0.7148 - val_accuracy: 0.7850\n",
            "Epoch 60/144\n",
            "63/63 [==============================] - 6s 89ms/step - loss: 0.2317 - accuracy: 0.9085 - val_loss: 0.5243 - val_accuracy: 0.8280\n",
            "Epoch 61/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.2264 - accuracy: 0.9080 - val_loss: 0.5355 - val_accuracy: 0.8230\n",
            "Epoch 62/144\n",
            "63/63 [==============================] - 6s 89ms/step - loss: 0.2461 - accuracy: 0.9045 - val_loss: 0.4683 - val_accuracy: 0.8170\n",
            "Epoch 63/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.2395 - accuracy: 0.9010 - val_loss: 0.5637 - val_accuracy: 0.8240\n",
            "Epoch 64/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.2119 - accuracy: 0.9130 - val_loss: 0.5019 - val_accuracy: 0.8390\n",
            "Epoch 65/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.2278 - accuracy: 0.9105 - val_loss: 0.4269 - val_accuracy: 0.8400\n",
            "Epoch 66/144\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.2056 - accuracy: 0.9145 - val_loss: 0.6281 - val_accuracy: 0.8300\n",
            "Epoch 67/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.2169 - accuracy: 0.9215 - val_loss: 0.4901 - val_accuracy: 0.8360\n",
            "Epoch 68/144\n",
            "63/63 [==============================] - 6s 87ms/step - loss: 0.2028 - accuracy: 0.9195 - val_loss: 0.4366 - val_accuracy: 0.8500\n",
            "Epoch 69/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.2023 - accuracy: 0.9215 - val_loss: 0.6487 - val_accuracy: 0.8320\n",
            "Epoch 70/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.1813 - accuracy: 0.9235 - val_loss: 0.7182 - val_accuracy: 0.8220\n",
            "Epoch 71/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1943 - accuracy: 0.9330 - val_loss: 0.5005 - val_accuracy: 0.8530\n",
            "Epoch 72/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.2107 - accuracy: 0.9170 - val_loss: 0.5621 - val_accuracy: 0.8470\n",
            "Epoch 73/144\n",
            "63/63 [==============================] - 6s 87ms/step - loss: 0.1856 - accuracy: 0.9315 - val_loss: 0.9365 - val_accuracy: 0.8030\n",
            "Epoch 74/144\n",
            "63/63 [==============================] - 5s 72ms/step - loss: 0.1995 - accuracy: 0.9155 - val_loss: 0.7239 - val_accuracy: 0.8340\n",
            "Epoch 75/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.1941 - accuracy: 0.9265 - val_loss: 0.5470 - val_accuracy: 0.8420\n",
            "Epoch 76/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.2037 - accuracy: 0.9240 - val_loss: 0.7013 - val_accuracy: 0.8150\n",
            "Epoch 77/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1649 - accuracy: 0.9345 - val_loss: 0.5795 - val_accuracy: 0.8240\n",
            "Epoch 78/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.1860 - accuracy: 0.9340 - val_loss: 0.5334 - val_accuracy: 0.8440\n",
            "Epoch 79/144\n",
            "63/63 [==============================] - 6s 91ms/step - loss: 0.1582 - accuracy: 0.9395 - val_loss: 0.6540 - val_accuracy: 0.8410\n",
            "Epoch 80/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.1932 - accuracy: 0.9335 - val_loss: 0.6092 - val_accuracy: 0.8370\n",
            "Epoch 81/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1954 - accuracy: 0.9290 - val_loss: 0.6915 - val_accuracy: 0.8400\n",
            "Epoch 82/144\n",
            "63/63 [==============================] - 6s 83ms/step - loss: 0.1837 - accuracy: 0.9330 - val_loss: 0.5643 - val_accuracy: 0.8370\n",
            "Epoch 83/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.1620 - accuracy: 0.9420 - val_loss: 0.6230 - val_accuracy: 0.8270\n",
            "Epoch 84/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.1465 - accuracy: 0.9470 - val_loss: 0.8531 - val_accuracy: 0.8310\n",
            "Epoch 85/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1723 - accuracy: 0.9360 - val_loss: 1.1750 - val_accuracy: 0.7870\n",
            "Epoch 86/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1845 - accuracy: 0.9345 - val_loss: 0.7474 - val_accuracy: 0.8380\n",
            "Epoch 87/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.1596 - accuracy: 0.9425 - val_loss: 0.9100 - val_accuracy: 0.8270\n",
            "Epoch 88/144\n",
            "63/63 [==============================] - 4s 67ms/step - loss: 0.1871 - accuracy: 0.9315 - val_loss: 0.6489 - val_accuracy: 0.7950\n",
            "Epoch 89/144\n",
            "63/63 [==============================] - 5s 83ms/step - loss: 0.1651 - accuracy: 0.9325 - val_loss: 0.5774 - val_accuracy: 0.8550\n",
            "Epoch 90/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1471 - accuracy: 0.9395 - val_loss: 0.8360 - val_accuracy: 0.8350\n",
            "Epoch 91/144\n",
            "63/63 [==============================] - 4s 68ms/step - loss: 0.1843 - accuracy: 0.9335 - val_loss: 0.7329 - val_accuracy: 0.8410\n",
            "Epoch 92/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.1621 - accuracy: 0.9345 - val_loss: 0.6744 - val_accuracy: 0.8410\n",
            "Epoch 93/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1563 - accuracy: 0.9350 - val_loss: 0.6803 - val_accuracy: 0.8370\n",
            "Epoch 94/144\n",
            "63/63 [==============================] - 6s 84ms/step - loss: 0.1636 - accuracy: 0.9420 - val_loss: 0.6504 - val_accuracy: 0.8510\n",
            "Epoch 95/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.1612 - accuracy: 0.9475 - val_loss: 0.9935 - val_accuracy: 0.8400\n",
            "Epoch 96/144\n",
            "63/63 [==============================] - 6s 87ms/step - loss: 0.1504 - accuracy: 0.9385 - val_loss: 1.1406 - val_accuracy: 0.8000\n",
            "Epoch 97/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.1754 - accuracy: 0.9410 - val_loss: 0.7208 - val_accuracy: 0.8420\n",
            "Epoch 98/144\n",
            "63/63 [==============================] - 6s 84ms/step - loss: 0.1287 - accuracy: 0.9570 - val_loss: 0.6631 - val_accuracy: 0.8450\n",
            "Epoch 99/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1555 - accuracy: 0.9465 - val_loss: 0.6561 - val_accuracy: 0.8390\n",
            "Epoch 100/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1581 - accuracy: 0.9415 - val_loss: 0.6358 - val_accuracy: 0.8420\n",
            "Epoch 101/144\n",
            "63/63 [==============================] - 6s 87ms/step - loss: 0.1489 - accuracy: 0.9465 - val_loss: 1.0013 - val_accuracy: 0.8180\n",
            "Epoch 102/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1887 - accuracy: 0.9320 - val_loss: 0.7291 - val_accuracy: 0.8320\n",
            "Epoch 103/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.1660 - accuracy: 0.9495 - val_loss: 0.6638 - val_accuracy: 0.8510\n",
            "Epoch 104/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.1900 - accuracy: 0.9400 - val_loss: 0.4668 - val_accuracy: 0.8540\n",
            "Epoch 105/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.1477 - accuracy: 0.9490 - val_loss: 0.8897 - val_accuracy: 0.8430\n",
            "Epoch 106/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.1514 - accuracy: 0.9470 - val_loss: 0.5978 - val_accuracy: 0.8440\n",
            "Epoch 107/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1436 - accuracy: 0.9470 - val_loss: 0.6833 - val_accuracy: 0.8550\n",
            "Epoch 108/144\n",
            "63/63 [==============================] - 6s 89ms/step - loss: 0.1664 - accuracy: 0.9445 - val_loss: 0.7045 - val_accuracy: 0.8300\n",
            "Epoch 109/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.1614 - accuracy: 0.9420 - val_loss: 0.5868 - val_accuracy: 0.8620\n",
            "Epoch 110/144\n",
            "63/63 [==============================] - 7s 100ms/step - loss: 0.1610 - accuracy: 0.9510 - val_loss: 0.5871 - val_accuracy: 0.8570\n",
            "Epoch 111/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.1563 - accuracy: 0.9490 - val_loss: 0.9488 - val_accuracy: 0.8540\n",
            "Epoch 112/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.1869 - accuracy: 0.9420 - val_loss: 0.9646 - val_accuracy: 0.8400\n",
            "Epoch 113/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1749 - accuracy: 0.9425 - val_loss: 0.7091 - val_accuracy: 0.8450\n",
            "Epoch 114/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1322 - accuracy: 0.9600 - val_loss: 0.6899 - val_accuracy: 0.8380\n",
            "Epoch 115/144\n",
            "63/63 [==============================] - 6s 88ms/step - loss: 0.1563 - accuracy: 0.9555 - val_loss: 1.0504 - val_accuracy: 0.8300\n",
            "Epoch 116/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1413 - accuracy: 0.9500 - val_loss: 1.0676 - val_accuracy: 0.8420\n",
            "Epoch 117/144\n",
            "63/63 [==============================] - 6s 91ms/step - loss: 0.1444 - accuracy: 0.9470 - val_loss: 1.0169 - val_accuracy: 0.8170\n",
            "Epoch 118/144\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.1621 - accuracy: 0.9425 - val_loss: 1.0634 - val_accuracy: 0.8310\n",
            "Epoch 119/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.1470 - accuracy: 0.9475 - val_loss: 0.9647 - val_accuracy: 0.8540\n",
            "Epoch 120/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1526 - accuracy: 0.9585 - val_loss: 0.9796 - val_accuracy: 0.8220\n",
            "Epoch 121/144\n",
            "63/63 [==============================] - 5s 80ms/step - loss: 0.1347 - accuracy: 0.9550 - val_loss: 1.0988 - val_accuracy: 0.8510\n",
            "Epoch 122/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.1677 - accuracy: 0.9520 - val_loss: 0.6794 - val_accuracy: 0.8640\n",
            "Epoch 123/144\n",
            "63/63 [==============================] - 6s 87ms/step - loss: 0.1647 - accuracy: 0.9465 - val_loss: 0.7527 - val_accuracy: 0.8570\n",
            "Epoch 124/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1578 - accuracy: 0.9500 - val_loss: 0.6672 - val_accuracy: 0.8580\n",
            "Epoch 125/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.1469 - accuracy: 0.9470 - val_loss: 0.8019 - val_accuracy: 0.8700\n",
            "Epoch 126/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1627 - accuracy: 0.9440 - val_loss: 0.7172 - val_accuracy: 0.8760\n",
            "Epoch 127/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1453 - accuracy: 0.9495 - val_loss: 1.2951 - val_accuracy: 0.8360\n",
            "Epoch 128/144\n",
            "63/63 [==============================] - 6s 87ms/step - loss: 0.1444 - accuracy: 0.9540 - val_loss: 0.7312 - val_accuracy: 0.8310\n",
            "Epoch 129/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.1697 - accuracy: 0.9510 - val_loss: 0.8467 - val_accuracy: 0.8480\n",
            "Epoch 130/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.1355 - accuracy: 0.9575 - val_loss: 0.7060 - val_accuracy: 0.8580\n",
            "Epoch 131/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.1437 - accuracy: 0.9555 - val_loss: 1.0724 - val_accuracy: 0.8390\n",
            "Epoch 132/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1638 - accuracy: 0.9505 - val_loss: 0.8760 - val_accuracy: 0.8450\n",
            "Epoch 133/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.1310 - accuracy: 0.9570 - val_loss: 1.2820 - val_accuracy: 0.8330\n",
            "Epoch 134/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1347 - accuracy: 0.9565 - val_loss: 1.2481 - val_accuracy: 0.8470\n",
            "Epoch 135/144\n",
            "63/63 [==============================] - 6s 88ms/step - loss: 0.1822 - accuracy: 0.9490 - val_loss: 0.9499 - val_accuracy: 0.8590\n",
            "Epoch 136/144\n",
            "63/63 [==============================] - 5s 70ms/step - loss: 0.1462 - accuracy: 0.9520 - val_loss: 0.7800 - val_accuracy: 0.8680\n",
            "Epoch 137/144\n",
            "63/63 [==============================] - 6s 88ms/step - loss: 0.2013 - accuracy: 0.9480 - val_loss: 0.7169 - val_accuracy: 0.8490\n",
            "Epoch 138/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1515 - accuracy: 0.9475 - val_loss: 0.9198 - val_accuracy: 0.8590\n",
            "Epoch 139/144\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.1719 - accuracy: 0.9560 - val_loss: 0.7725 - val_accuracy: 0.8650\n",
            "Epoch 140/144\n",
            "63/63 [==============================] - 5s 71ms/step - loss: 0.1628 - accuracy: 0.9530 - val_loss: 0.8836 - val_accuracy: 0.8600\n",
            "Epoch 141/144\n",
            "63/63 [==============================] - 6s 88ms/step - loss: 0.1486 - accuracy: 0.9555 - val_loss: 0.9633 - val_accuracy: 0.8580\n",
            "Epoch 142/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1243 - accuracy: 0.9615 - val_loss: 1.0941 - val_accuracy: 0.8440\n",
            "Epoch 143/144\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.1985 - accuracy: 0.9390 - val_loss: 0.8657 - val_accuracy: 0.8510\n",
            "Epoch 144/144\n",
            "63/63 [==============================] - 5s 69ms/step - loss: 0.1509 - accuracy: 0.9510 - val_loss: 1.0535 - val_accuracy: 0.8470\n"
          ]
        }
      ],
      "source": [
        "PARTIAL_RUN = False\n",
        "epochs = 144\n",
        "if PARTIAL_RUN:\n",
        "  epochs = 2\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=def_callbacks(\"convnet_from_scratch_with_augmentation.keras\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LAK18JWiwNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230c91e2-3750-44c2-d2cf-84d6ddc3c014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 4s 49ms/step - loss: 0.4372 - accuracy: 0.8270\n",
            "Test accuracy: 0.827\n"
          ]
        }
      ],
      "source": [
        "test_model = keras.models.load_model(\n",
        "            \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU64Kt9J7Ha4"
      },
      "source": [
        "With data augmentation, we roughly get **82-85%** accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the size of the model"
      ],
      "metadata": {
        "id": "SU-TaUfJkcHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_model_size = Path(\"convnet_from_scratch_with_augmentation.keras\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained model size: {pretrained_model_size} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgThjJpYkgmE",
        "outputId": "36eb5513-70e1-4222-eeab-f43a774d1464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained model size: 7 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the prediction function"
      ],
      "metadata": {
        "id": "b59L7tsS9sqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer"
      ],
      "metadata": {
        "id": "ElT7Bizw-amZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = test_model"
      ],
      "metadata": {
        "id": "hazN0wci-C85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(img):\n",
        "\n",
        "    # Start the timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # Reading the image and size transformation\n",
        "    features = Image.open(img)\n",
        "    features = features.resize((180, 180))\n",
        "    features = np.array(features).reshape(1, 180,180,3)\n",
        "\n",
        "    # Create a prediction label and prediction probability dictionary for each prediction class\n",
        "    # This is the required format for Gradio's output parameter\n",
        "    pred_labels_and_probs = {'dog' if MODEL.predict(features)> 0.5 else 'cat':float(MODEL.predict(features))}\n",
        "\n",
        "    # Calculate the prediction time\n",
        "    pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "    # Return the prediction dictionary and prediction time\n",
        "    return pred_labels_and_probs, pred_time"
      ],
      "metadata": {
        "id": "F-R1-gKQ83QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict('/content/cats_vs_dogs_small/test/cat/cat.1502.jpg')"
      ],
      "metadata": {
        "id": "eO6qfg7mGeNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "qhui828xGGJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"Classification Demo\"\n",
        "description = \"Cat/Dog classification Tensorflow model with Augmentted small dataset\"\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
        "                    inputs=gr.Image(type='filepath'), # what are the inputs?\n",
        "                    outputs=[gr.Label(label=\"Predictions\"), # what are the outputs?\n",
        "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
        "                    #examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "vh3BsTpU7NUz",
        "outputId": "acccec09-0fff-4006-ac40-6e098232ab30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e73fd6212f894e5fba.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e73fd6212f894e5fba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File structure for uploading on HuggingFace\n",
        "To upload our demo Gradio app, we'll want to put everything relating to it into a single directory.\n",
        "\n",
        "For example, our demo might live at the path `demo/binary_Classification/` with the file structure:\n",
        "\n",
        "```\n",
        "demo/\n",
        "# └── binary_Classification/\n",
        "    ├── convnet_from_scratch_with_augmentation.keras\n",
        "    ├── app.py\n",
        "    ├── examples/\n",
        "    │   ├── example_1.jpg\n",
        "    │   ├── example_2.jpg\n",
        "    │   └── example_3.jpg\n",
        "    ├── ____.py\n",
        "    └── requirements.txt\n",
        "```\n",
        "\n",
        "Where:\n",
        "* `convnet_from_scratch_with_augmentation.keras` is our trained Tensorflow model file.\n",
        "* `app.py` contains our Gradio app (similar to the code that launched the app).\n",
        "    * **Note:** `app.py` is the default filename used for Hugging Face Spaces, if you deploy your app there, Spaces will by default look for a file called `app.py` to run. This is changable in settings.\n",
        "* `examples/` contains example images to use with our Gradio app.\n",
        "* `model.py` contains the model defintion as well as any transforms assosciated with the model.\n",
        "* `requirements.txt` contains the dependencies to run our app such as `tensorflow`, `numpy` and `gradio`.\n",
        "\n"
      ],
      "metadata": {
        "id": "MktXHLioMnZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps for Uploading Classification app on Huggingface spaces"
      ],
      "metadata": {
        "id": "IxBzPojPRuon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Note:** The following series of steps uses a Git (a file tracking system) workflow.\n",
        "\n",
        "1. [Sign up](https://huggingface.co/join) for a Hugging Face account.\n",
        "2. Start a new Hugging Face Space by going to your profile and then [clicking \"New Space\"](https://huggingface.co/new-space).\n",
        "    * **Note:** A Space in Hugging Face is also known as a \"code repository\" (a place to store your code/files) or \"repo\" for short.\n",
        "3. Give the Space a name, for example, mine is called `Ramendra/image_classification`, you can see it here: https://huggingface.co/spaces/Ramendra/image_classification\n",
        "4. Select a license (I used [MIT](https://opensource.org/licenses/MIT)).\n",
        "5. Select Gradio as the Space SDK (software development kit).\n",
        "   * **Note:** You can use other options such as Streamlit but since our app is built with Gradio, we'll stick with that.\n",
        "6. Choose whether your Space is it's public or private (I selected public since I'd like my Space to be available to others).\n",
        "7. Click \"Create Space\".\n",
        "8. Clone the repo locally by running something like: `git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]` in terminal or command prompt.\n",
        "    * **Note:** You can also add files via uploading them under the \"Files and versions\" tab.\n",
        "9. Copy/move the contents of the application files/folder to the cloned repo folder.\n",
        "\n",
        "  `git remote -v`\n",
        "\n",
        "10. To upload and track larger files (e.g. files over 10MB or in our case, our PyTorch model file) you'll need to [install Git LFS](https://git-lfs.github.com/) (which stands for \"git large file storage\").\n",
        "\n",
        "  `git lfs install`\n",
        "\n",
        "  Track the files over 10MB with Git LFS with `git lfs track \"*.file_extension\"`.\n",
        "\n",
        "  `git lfs track \"convnet_from_scratch_with_augmentation.keras\"`\n",
        "\n",
        "\n",
        "13. Track `.gitattributes` (automatically created when cloning from HuggingFace, this file will help ensure our larger files are tracked with Git LFS). You can see an example `.gitattributes` file on the spaces repo unders files.\n",
        "    * `git add .gitattributes`\n",
        "\n",
        "14. Add the rest of the `foodvision_mini` app files and commit them with:\n",
        "    * `git add *`\n",
        "    * `git commit -m \"first commit\"`\n",
        "15. Push (upload) the files to Hugging Face:\n",
        "    * `git push`\n",
        "16. Wait 3-5 minutes for the build to happen (future builds are faster) and your app to become live!\n",
        "\n",
        "If everything worked, you should see a live running example of our classification demo like the one here: https://huggingface.co/spaces/Ramendra/image_classification"
      ],
      "metadata": {
        "id": "6T0wvfx_Spq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook for creating app.py file\n",
        "\n",
        "This [Notebook](https://colab.research.google.com/drive/13X2E9v7GxryXyT39R5CzxrNwxfA6KMFJ?usp=sharing) contains only trained model uploading and inference scripts along with Gradio implementation. **No training scripts.**"
      ],
      "metadata": {
        "id": "QyNKr3qiyvGg"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}